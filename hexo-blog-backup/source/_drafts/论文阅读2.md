---
abbrlink: '0'
---
# 论文阅读2：《Efficient Estimation of Word Representations in Vector Space》--Word2Vec

> 基于神经网络的词向量（基于TensorFlow实现）--- 向量空间中词表示的有效估计
>
> 作者：Tomas Mikolov
>
> 单位：Google
>
> 论文来源：ICLR 2013



## 论文导读

### 语言模型

#### 概念

语言模型：预测每个句子在语言中出现的概率。P(S)被称为语言模型，即用来计算一个句子概率的模型。公式如下：
$$
P(S) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)...P(w_n|w_1,w_2,...,w_{n-1})
$$
计算：$P(w_i|w_1,w_2,...,w_{i-1}) = P(w_1,w_2,...,w_i) / P(w_1,w_2,...,w_{i-1})$

这样计算的缺点：数据过于稀疏、参数空间太大

解决办法：基于马尔科夫假设，即下一个词的出现仅依赖于它前面的一个或几个词。

假设下一个词的出现依赖它前面的一个词，则有：$P(S) = P(w_1)P(w_2|w_1)P(w_3|w_2)...P(w_n|w_{n-1})$



#### n-gram模型

n-gram模型：假设当前词的出现概率只与它前面的**N-1**个词有关。

如何选择n：

- 更大的n：对于下一个词出现的约束信息更多，具有更大的**辨别力**
- 更小的n：在训练语料库中出现的次数更多，具有更可靠的统计信息，具有更高的**可靠性**

理论上，n越大越好，经验上，trigram（N=3）用的最多，尽管如此，原则上，能用bigram（N=2）解决，绝不使用trigram。



构造语言模型：最大似然估计 + 平滑，公式如下：
$$
P(w_i|w_1,w_2,...,w_{i-1}) = \frac{Count(w_1,w_2,...,w_{i-1},w_i)}{Count(w_1,w_2,...,w_{i-1})}
$$


### 词向量简介

#### 独热编码

概念：对应的词所在的位置设为1，其他为0

缺点：

- 语义鸿沟问题：不能捕捉到词与词之间的相似度关系
- 维数灾难、稀疏：维度特别大
- 无法表示未出现的词汇



#### 分布式表示--词向量（Word Embedding）

词向量表示的核心：利用上下文信息进行词的表示

- 词表示：[0.792,-0.177,...]
- 常见维度为50或100
- 解决语义鸿沟问题
- 可以通过计算向量之间的距离（欧式距离、余弦距离等）来体现词与词的相似性



#### 如何训练词向量

通过语言模型训练词向量



### 前期知识储备

- 语言模型：了解基本的语言模型知识，掌握语言模型的计算方法
- 词向量：了解词向量的概念和种类，掌握词向量不同表示的优缺点
- 概率论：了解基本的概率论知识，掌握条件概率的概念和公式
- 循环神经网络：了解循环神经网络的结构，掌握它的基本工作原理



## 论文精读

### 论文整体框架

- 摘要：论文高度概括，包含作者研究思路
- 1.介绍：简述论文背景，提出写作目的，介绍前人工作
- 2.模型结构：神经网络语言模型和循环神经网络语言模型
- 3.新的对数线性模型
- 4.结果
- 5.学习得到的关系示例
- 6.结论
- 7.后续工作



### 神经语言模型

#### 概念

神经网络语言模型（NNLM）：直接从语言模型出发，将模型最优化过程转化为求词向量表示的过程

- 目标函数：$L(\theta) = \sum_t log P(w_t|w_{t-n+1},...,w_{t-1})$

- 根据前n-1个单词，预测第t个位置单词的概率
- 使用了非对称的前向窗函数，窗长度为n-1
- 滑动窗口遍历整个语料库求和，计算量正比于语料库大小
- 概率P满足**归一化条件**，这样不同位置t处的概率才能相加，即$\sum_t P(w|w_{t-n+1},...,w_{t-1}) = 1$



#### 训练

通过投影矩阵，也是稠密词向量表示，将One-Hot编码的词通过投影层进行转化，再经过神经网络和非线性激活函数计算，各层权重优化是通过BP+SGD，最后通过softmax得出最大的值。

- 输入层（input）：输入（N-1）个前向词，One-Hot表示
- 投影层（projection layer）：采用线性投影方式将词向量投影到稠密D维表示
- 隐藏层（hidden layer）：做全连接，全连接神经元数量用户自定
- 输出层（output）：softmax分类器

每个训练样本的计算复杂度：$Q = N*D + N*D*H + H*V$

一个简单模型在大数据量上的表现比复杂模型再少数据量上的表现会好



#### 循环神经网络语言模型

循环神经网络语言模型（RNNLM）：基于循环神经网络的语言模型

- w(t)表示第t个时刻的当前输入单词，维度为V，V是词典大小，One-hot表示
- s(t-1)代表隐层的前一次输出
- y(t)表示输出

计算复杂度：$Q = H*H + H*V$



#### 模型缺点

- 计算复杂度过大
- 参数较多



### CBOW（连续词袋）模型

CBOW（Continuous Bag of Words），连续词袋模型，即利用中心词（Wt）的上下文（context）来预测中心词（Wt）。

- 目标函数：$L = \sum_{w\epsilon C} log p(w|Context(w))$
- 无隐层
- 使用**双向**上下文窗口
- 上下文词序**无关**（BoW）
- 输入层直接使用**低维稠密向量**表示
- 投影层简化为**求和**（平均）



训练技巧

- 层次softmax
- 负采样（Negative Sampling）



### Skip-gram（跳字）模型

Skip-gram，跳字模型，是根据中心词（Wt）来预测周围的词，即预测上下文（context）。

- 目标函数：$L = \sum_{w\epsilon C} log p(Context(w)|w)$
- 输入层：只含当前样本的中心词w的词向量
- 投影层：恒等投影，为了和CBOW模型对比
- 输出层：和CBOW模型一样，输出层也是一颗Huffman树



### 实验和结果

#### 实验评估任务

word similarity task

- 相似度任务，目的是评估词向量模型在两个词之间的语义紧密度和相关性的能力

- 评价指标：斯皮尔曼等级相关系数
  $$
  \rho = \frac{\sum_i (x_i - \bar x)(y_i - \bar y)}{\sqrt{\sum_i (x_i - \bar x)^2 \sum_i (y_i - \bar y)^2}}
  $$



word analogy task

- 词汇类比任务，考察了用词向量来推断不同单词之间的语义关系的能力

- 举例

  vec(法国) - vec(巴黎) = answer_vector - vec(罗马)

  answer_vector = vec(法国) - vec(巴黎) + vec(罗马)



### 讨论与总结

#### 讨论

存在问题

- 对每个局部上下文窗口单独训练，没有利用包含在全局共现矩阵中的统计信息
- 对多义词无法很好地表示和处理，因为使用了唯一的词向量



解决方式

- Glove：利用全局信息编码词向量



#### 总结

论文主要创新点

- 提出两种从大规模数据集中计算连续向量表示的模型
- 能在较少的资源上进行运算
- 在大规模语料上得到高质量的词向量



## 代码复现

### 准备工作

#### 运行代码

- 代码结构简单，各模型只含有一个运行文件
- 便于初学者理解代码实现思想
- 采用TensorFlow框架
- 由TensorFlow源码的示例程序改进



#### 数据集

论文原文：Google News，数据集体量较大

示例程序：http://mattmahoney.net/dc/ 网站中较小的语料库text8.zip

中文词向量：https://github.com/Embedding/Chinese-Word-Vectors



#### 实现流程

1. 下载数据集
2. 制作词表
3. 生成训练样本
4. 定义模型
5. 执行训练



### Skip-gram模型实现



### CBOW模型实现



### 总结

