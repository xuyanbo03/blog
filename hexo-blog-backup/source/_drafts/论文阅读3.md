---
abbrlink: '0'
---


# 论文阅读3：《Distributed representations of sentences and docments》

> 句子和文档的分布式表示学习
>
> 作者：Quoc Le and Tomas Mikolov
>
> 单位：Google
>
> 论文来源：ICML 2014

## 论文导读

### 句子分布式表示简介

句子分布式表示：句子的分布式表示就是将一句话或者一段话（这里将句子和文档同等看待，文档相当于较长的句子）用**固定长度的向量**表示。

意义：如果能够用一个向量准确地表示一句话，那么可以直接用这个向量用于文本分类、信息检索、机器翻译等待各个领域。



### 句子分布式表示相关方法

#### 历史模型

基于统计的句子分布式表示

- Bag-of-words算法

  1. 构建一个**词表**，词表中每个元素都是一个词
  2. 对于一句话s，统计词表中每个词在s中出现的次数
  3. 根据词表中每个词在s中出现的次数，构造一个**词表大小的向量**

- Bag-of-n-grams算法

  同词袋模型类似，词表中的元素可以为词也可以为n-gram短语



基于深度学习的句子分布式表示

- 加权平均法

  1. 构建**词表**，词表中每个元素都是词

  2. 使用词向量学习方法（skip-gram等）学习每个词的词向量表示

  3. 对句子s中的每个词（$w_1,w_2,...,w_n$）对应的词向量（$e_1,e_2,...,e_n$）加权平均，结果为句子s的分布式表示：
     $$
     e_s = \frac{1}{n}\sum_{i=1}^ne_i
     $$
     

- 深度学习模型

  1. 构建**词表**，词表中每个元素都是词
  2. 使用词向量学习方法（skip-gram等）学习每个词的词向量表示
  3. 将句子s中的每个向量作为**输入**送进**深度神经网络**（CNN or RNN），然后通过**监督学习**，学习每个句子的分布式表示



### 前期知识储备

- 熟悉词向量的相关知识：了解**词向量矩阵**，以及词向量学习到的语法以及语义信息的含义
- 了解使用语言模型训练词向量的方法：了解通过**语言模型**来训练词向量的模型



### 课程安排和学习建议

#### 课程安排

导读-精读-复现-总结



#### 学习建议

- 储备知识：学习预备知识
- 图文结合：先看论文中模型的图，再看文章
- 精读论文：了解论文的动机，创新点，模型，实验和解释性
- 粗细有度：不必每处都看，重点看abstract,model,experience



## 论文精读

### 论文整体框架

- 摘要
  - 句向量表示的概念和意义
  - 以往的句向量表示模型以及它的缺点
  - 本文提出的模型和该模型的优点
  - 所提出模型的效果
- 1.介绍
- 2.句子分布式表示模型
- 3.实验
- 4.相关工作
- 5.结论



### 传统经典算法模型

#### Bag-of-words

##### 算法过程

1. 构建一个**词表**，词表中每个元素都是一个词
2. 对于一句话s，统计词表中每个词在s中出现的次数
3. 根据词表中每个词在s中出现的次数，构造一个**词表大小的向量**

##### 模型缺点

1. 因为是词袋模型，所以丢失了词之前的位置信息
2. 句向量只是单纯地利用了统计信息，而没有得到语义信息，或者只得到很少的语义信息



#### Bag-of-n-grams

##### 算法过程

同词袋模型类似，词表中的元素可以为词也可以为n-gram短语

##### 模型缺点

1. 因为使用了n-gram，所以保留了位置信息，但是n-gram不会太大，最多是4-gram，所以只保留了很少的位置信息
2. n-gram同样没有学习到语义信息



#### 加权平均法

##### 算法过程

1. 构建**词表**，词表中每个元素都是词

2. 使用词向量学习方法（skip-gram等）学习每个词的词向量表示

3. 对句子s中的每个词（$w_1,w_2,...,w_n$）对应的词向量（$e_1,e_2,...,e_n$）加权平均，结果为句子s的分布式表示：
   $$
   e_s = \frac{1}{n}\sum_{i=1}^ne_i
   $$

##### 模型缺点

对**所有的词向量进行平均**，丢失了**词之前的顺序信息**以及词与词之间的关系信息



#### 深度学习模型

##### 算法过程

1. 构建**词表**，词表中每个元素都是词
2. 使用词向量学习方法（skip-gram等）学习每个词的词向量表示
3. 将句子s中的每个向量作为**输入**送进**深度神经网络**（CNN or RNN），然后通过**监督学习**，学习每个句子的分布式表示

##### 模型缺点

只能使用**标注数据**训练每个句子的句向量，这样训练得到的向量都是**任务导向的**，不具有通用性



#### 基于语言模型的词向量训练方法

##### 概念

语言模型：语言模型可以给出每个句子是句子的概率：越大越好
$$
P(s) = \prod_{i=1}^{T}P(w_i)
$$
而每个词的概率定义成n-gram形式，即每个词出现只与前n-1个词有关：
$$
P(w_t) = P(w_t|w_{t-n+1}^{t-1})
$$
评价语言模型好坏的**指标困惑度**（perpiexiy）：越小越好
$$
PP(s) = \sqrt[T]{1/P(s)}
$$

$$
PP(s) = e^{-\frac{1}{T}\sum_{i=1}^T logP(w_i)}
$$



##### 算法过程

1. 对于每个词随机初始化一个词向量
2. 取得一个句子连续的n-1个词，将这n-1个词对应的词向量连接（concatenate）在一起形成向量e
3. 将e作为输入，送入一个单隐层神经网络，隐层的激活函数为tanh，输出层的神经元个数为词表大小



##### 模型缺点

训练出一组词向量，又得到了一个语言模型。其次不需要标注数据，可以使用很大的数据集。



### 论文提出改进的模型

#### 分布式句向量训练模型

##### 算法过程

1. 类似于前面提到的基于语言模型的词向量训练模型，这里的句向量训练模型也是利用前几个词预测后一个词
2. 不同的是，这里将每句话映射成一个句向量，联合预测后一个词出现的概率，这里就学习到了每个词的词向量和每句话的句向量

##### 训练阶段

通过训练集构建词表，并随机初始化词向量矩阵W和训练集的句向量矩阵D，设置n-gram，文中为窗口大小，然后利用句向量训练模型训练矩阵模型的所有参数，包括词向量矩阵和句向量矩阵。最后将学习到的句向量用于分类器预测句子的类别概率。

##### 测试阶段

固定词向量矩阵W和模型的其他参数，重新构建句向量矩阵D并随机初始化，然后利用梯度下降训练矩阵D，从而得到测试集每个句子的句向量。



#### 无序句向量训练模型

忽略词序信息的模型

##### 算法过程

每个句子通过随机初始化句向量矩阵映射成一个句向量，然后通过句向量每次随机预测句子中的一个词。然后将学习到的句向量送到已经训练好的分类器，预测句子的概率。

本文分别使用提出的两种模型训练得到两个句向量，然后将两个句向量合并（concatenate），得到最终的句向量表示。



### 实验和结果

#### 数据集

SST：斯坦福提供的情感分析数据集（0-1之间的情感值），其中训练数据8544条，测试数据2210条，验证数据1101条，其中还包括将所有数据转化为短句共239232条。

IMDB：IMDB的评论数据，其中训练集25000条，12500条正例，12500条负例。测试集25000条，12500条正例，12500条负例。还有50000条未标注数据。

评价方法：SST数据集可以根据情感值分为5类，采用的分类器是Logistic回归，也可以分为两类，评价指标就是预测情感类别的错误率，越低越好。IMDB数据集是一个二分类任务，采用的分类器是单隐层神经网络，评价指标为预测情感类别的错误率，同样越低越好。



#### 实验结果

本文提出的句向量方法优于朴素贝叶斯，SVM，词向量平均法以及神经网络方法，在二分类和五分类任务都取得state-of-the-art的结果。



### 讨论和总结

####  讨论

- 目前主流的句向量表示方法？

  基于神经网络的句向量学习方法，使用预训练的词向量以及神经网络可以得到非常好的句向量表示

- 测试过程还需要训练，大大降低了效率？

  使用基于神经网络的句向量学习方法，当前流行的ELMO，BERT

- 是否有其他句向量训练方法？

  后人提出基于seq2seq模型的句向量训练方法



#### 总结创新点

- 提出了一种新的无监督的句向量训练方法
- 可以直接用于下游任务
- 在论文发表的时候取得了SOTA结果



## 代码复现

### 学习流程

- 任务定义
  - 搞清楚程序的目的是什么
  - 为了实现什么任务
- 数据来源
  - 源码获取渠道
  - 数据集类型
  - 数据集的来源
- 运行环境
  - 运行环境
  - 实验工具
  - 第三方库
- 运行结果
  - 能否运行成功
  - 运行代码后出现什么样的结果
  - 结果的形式是什么
- 如何实现
  - 代码整体架构
  - 每部分实现细节



### 语料的预处理过程

1. 切词：使用NLTK工具将文档分成一个一个的词

   将一段话切分成一个个的词

2. 统计单词：统计语料中出现的单词频率并根据频率构建词表

   使用统计所有的词，然后使用collections的Counter统计每个词的频率，然后构建一个含有30000个词的词表，其中包含pad字符和unk字符

3. 分配ID：为每一个单词分配一个ID

4. 编号表示：将程序所需数据集文本转化为用单词编号的形式表示

   将所有文本中的词利用构建的词表转化为编号

5. 窗口数据集构建：按照窗口大小构建本次模型训练所需要的数据集

   构建大小为10的窗口，进行数据集构建



### 模型训练

1. 训练集句向量和词向量训练
2. 分类器训练
3. 测试集句向量训练
4. 测试集评估

