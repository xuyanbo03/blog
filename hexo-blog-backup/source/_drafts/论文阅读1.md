---
abbrlink: '0'
---
# 论文阅读1：《Deep Learning》综述

## 作者简介

2019年3月图灵奖获得者

- Yoshua Bengio
- Geoffrey Hinton
- Yann LeCun



## 前期知识储备

### 应用数学和机器学习基础

- 线性代数：本科线性代数
- 概率论与信息论：本科概率论知识
- 数值计算：梯度优化、约束优化等
- 机器学习基础：过拟合、欠拟合、超参数等



### 面试题举例

1. 概率题

   一段1米长的绳子，随机切两刀，分成三段，求能够组合成一个三角形的概率？

2. 机器学习基础

   - 什么是Bias？什么是Variance？
   - 如何解决过拟合和欠拟合的问题？



## 课程安排和学习建议

>  路线规划和参考资料

### 课程要求

- 记录每一天学习笔记
- 随时记录不明白的问题：记得提问弄明白
- 按要求完成作业



### 学习建议

- 精读经典论文：读综述型论文，再读方向经典论文
- 跟进最新论文：了解前沿发展动态
- 通读领域论文：实现量变到质变
- 复现领域经典论文：代码实现想法
- 论文写作：实验结果、新想法等写论文



### 一篇论文学习时长的建议

> 任何新技能的系统学习和自我约束和管理都是成败关键。

- 论文学习周期：1周
- 分成四个阶段学习
  - 预习阶段：导读30min
  - 理论学习：论文讲解1h
  - 代码实践：代码讲解1h
  - 总结答疑：总结1h
- 每个阶段1小时以上集中学习



## 深度学习背景介绍

> 论文第一节介绍

### 学习方法

传统方法：构建一个模式识别或机器学习系统需要领域专家的帮助设计一个特征抽取器

表示学习：一系列允许机器接受原始数据并自动转换成监测和分类任务能处理的形式的方法

深度学习方法：多层表示的**表示学习方法**，由简单的非线性模块组成，上一层的转换结果作为下一层的输入继续进行转换表示，提升抽象层次。

深度学习的关键：特征层不是专家工程师设计，而是使用学习算法从数据中学到的



### 深度前馈网络基本结构

- 输入层：输入实例的特征向量
- 隐藏层：中间计算结果输出
- 输出层：学习的目标结果向量
- 全连接
- 权重参数
- 激活函数
- **反向传播**



### 应用方向

- 图像、视频
- 语言、语音
- 序列数据预测
- 推荐系统



## 监督学习

> 论文第二节介绍

### 训练步骤

1. 收集不同类别的图片，打标签

2. 在训练过程中，对输入图片输出一个得分向量，每个类别均有得分

3. 希望目标类别在所有类别中拥有最高得分

4. 设计目标函数，计算网络输出和目标之间的差距

5. 随后通过修改中间参数（权重）来最小化误差

   最常用优化手段：随机梯度下降（SGD）



### 总结

传统机器学习需要精心设计特征抽取器，而这个过程往往需要大量的专业技能和丰富的领域知识。深度学习则可以避免繁琐的过程，特征可以通过学习算法自动学习得到，通过多个非线性层组合（5-20层），系统可以敏锐观察到图像中的关键部分，忽略非关键部分。



## 反向传播

> 论文第三节介绍

### 概念

反向传播是计算目标函数 $J(\theta)$ 对多层神经网络权重的梯度，利用微积分中的链式法则完成反向梯度计算。



### 面试题举例

- 写出反向传播的推导公式？



### 激活函数

- ReLU：目前最常用的激活函数 $f(z) = max(z, 0)$
- Sigmoid：$\frac{1}{1+e^{-z}}$
- tanh：$tanh(z)$



## CNN和基于CNN的图像理解

### 卷积神经网络基础

#### 输入信号的维度

1D：序列和信号，包括语言信息等

2D：平面图像、声音频谱图等

3D：视频信号、立体图像等。



#### 卷积神经网络的关键

1. 信号的局部连接
2. 共享权重
3. 降采样
4. 多层网络结构



#### 卷积神经网络主要层次结构

- 卷积层
  - 卷积的输出被称作特征**映射（Feature Map）**
  - **卷积核（Kernel）**共享权重，减少了参数量
  - 卷积神经网络具有**稀疏交互性（Sparse Interactions）**
  - 这样设计的好处：
    - 数组形式的数据（例如图像），局部值之间是高度相关的，形成容易检测的各种局部图形
    - 图片和其他信号数据的局部统计特征具有位置不变性

- 池化层
  - 将邻域内**语义相近**的特征进行融合
  - 常用池化操作：Max-pool、Average-pool
  - 当前一层特征组件发生位置变化或表现变化，降低当前层的表征变化



卷积层和池化层来自于视觉神经科学中简单细胞和复杂细胞的经典概念。

通常将**2-3个卷积层+非线性激活函数+池化层**作为一个模块，一个模型通常包含多个这种模块。



#### 卷积神经网络多级结构的功能

1. 图像**低级特征**到**高级特征**：边缘->纹理->组件->物体
2. 接近输入的特征图：检测边缘、简单纹理
3. 层次加深：语义信息提升，理解组件或物体的表征



#### 面试题举例

1. 如何计算CNN的参数量？
2. 为什么需要添加非线性激活函数？线性的激活函数会怎样？
3. 如何计算多层卷积、池化网络每一层的感受野（Receptive Field）？



### 经典卷积神经网络

LeNet：来自Yann Lecun，卷积神经网络的开山之作，用于解决手写数字识别的视觉任务

AlexNet：在2012年ImageNet竞赛中以超过第二名10.9个百分点的绝对优势一举夺冠

VGG：由牛津大学VGG组提出，2014年ImageNet竞赛定位任务第一名和分类任务第二名

GoogleNet：2014年ImageNet分类任务上击败VGG-Net夺得冠军

ResNet：2015年何凯明提出的ResNet在ISLVRC和COCO上横扫所有选手获得冠军

DenseNet：CVPR2017最佳论文，在扩宽网络宽度上也能提升精度



### 基于CNN的图像理解

#### 背景

ImageNet大赛之前，卷积神经网络的作用和能力一直被忽视，2012年ImageNet大赛上用CNN识别1000个类别近100万张图片，错误率比之前最好成绩降低了近一半，成功也来自GPU、ReLU以及一个新的正则化方法Dropout的发明，基于CNN的视觉系统的表现引起了大多数技术公司的注意，包括Google、Facebook、Microsoft、IBM等，许多公司包括NVIDA、Inter、Mobileye、Qualcomm和Samsung正在开发卷积神经网络芯片，支持在智能手机、数码相机、机器人和自动驾驶上的实时视觉应用。



#### 应用

人脸识别

- 利用CNN提取人脸特征向量，与人脸库中的人脸进行判别，返回最相似的
- FaceNet、DeepFace、SphereFace等



服装识别

- 利用CNN网络提取图像中服装特征，单分类或多分类完成服装属性的识别
- DeepFashion数据集



## 分布式特征表示和语言处理、循环神经网络、未来

### 分布式特征表示和语言处理

#### 概念

**分布式特征表示**是深度学习的一个核心概念：发现数据之间的语义相似性

深度网络两个巨大优势

1. 分布式特征表示提升算法重新组合学过的特征的泛化能力
2. 深度网络特征表示组成的网络能够带来例如指数级深度的其他优势



示例：预测语句中下一个单词

1. 输入：文本内容，每个单词输入到网络中为1/N的向量
2. 输出：下一个单词是什么
3. 学习：word-vector，网络挖掘深层语义特征，作为输入-输出的连接关系

从文本中学习到的单词的向量表示在自然语言处理中广泛使用。

那能不能把稀疏的词向量的维度变小呢？

Word-Embedding方法：将高维词向量嵌入到一个低维空间，即用分布式特征表示方法表示的较短词向量，较容易的分析词之间的关系。



#### Word2Vec

- 简单化的神经网络
- 输入是One-Hot Vector
- Hidden Layer 没有激活函数，也就是线性的单元
- Output Layer维度等于Input Layer维度，使用Softmax回归
- 训练后只需要隐层的权重矩阵
- 分为CBOW和Skip-gram两种模型



#### N-grams方法

- 在分布式表示特征学习之前广泛使用
- 指文本或语音中连续出现的n个“部分”
- N元语法是基于（N-1）阶马尔可夫链的一种概率语言模型
- “部分”通常包括：音素、音节、字母、单词或基本词组等
- 当N=1,2,3时，分别称为“一元语法”、“二元语法”、“三元语法”
- 还可以用于：计算字符串距离



### 循环神经网络

#### RNN

- 适用于序列化输入，如语音和语言
- 一次处理一个输入序列元素
- 维护隐单元中的”状态向量“，这个向量隐式地包含过去时刻序列元素的历史信息
- 训练过程中反传梯度在每一个时刻会增长或下降，长时间迭代后会出现梯度爆炸或消失
- 基于其网络结构和训练的特点，RNNs在预测文本中下一个字符或序列中下一个单词这两个方面具有很好地表现
- RNNs也可以应用于更加复杂的任务中
- 一旦展开，可以把它当做一个所有层共享权值的前馈神经网络
- 理论上和经验上的证据都证明很难学习并长期保存信息



#### LSTM

- 输入门、遗忘门、记忆单元、输出门
- 遗忘门：控制是否遗忘，以一定概率控制是否遗忘上一层的隐藏细胞状态
- 输入门：处理当前序列位置的输入
- 细胞状态：前面的遗忘门和输入门的结果都会作用于细胞状态C(t)



### 未来展望

#### 无监督学习

|          | 方法         | 模型                 | 策略                 | 算法                 |
| -------- | ------------ | -------------------- | -------------------- | -------------------- |
| 聚类     | 层次聚类     | 聚类树               | 类内样本距离最小     | 启发式算法           |
|          | K均值聚类    | K中心聚类            | 样本与类中心距离最小 | 迭代算法             |
|          | 高斯混合模型 | 高斯混合模型         | 似然函数最大         | EM算法               |
| 降维     | PCA          | 低维正交空间         | 方差最大             | SVD                  |
| 话题分析 | LSA          | 矩阵分解模型         | 平方损失最小         | SVD                  |
|          | NMF          | 矩阵分解模型         | 平方损失最小         | 非负矩阵分解         |
|          | PLSA         | PLSA模型             | 似然函数最大         | EM算法               |
|          | LDA          | LDA模型              | 后验概率估计         | 吉布斯抽样，变分推理 |
| 图分析   | PageRank     | 有向图上的马尔可夫链 | 平稳分布求解         | 幂法                 |



- 无监督学习对于重新点燃深度学习的热潮起到了促进作用
- 有监督学习比无监督学习更加成功，但是在人类和动物的学习中无监督学习占据主导地位
- 无监督学习问题可以进一步分类聚类问题和关联问题
  - 聚类问题：希望在数据中发现内在的分组，比如以购买行为对顾客进行分组
  - 关联问题：想发现数据的各部分之间的联系和规则，例如购买X物品的顾客也喜欢购买Y物品



#### 强化学习

机器给一个动作a到环境中，环境反馈一个状态x和奖赏r给机器，机器根据状态x和奖赏r不断的更新动作。



强化学习通常用马尔可夫决策过程来描述：

1. 机器处于环境E中，状态空间为X，其中每个状态x属于X是机器感知到的环境的描述
2. 机器能采取的动作构成动作空间A
3. 转移函数P将使得环境从当前状态按某种概率转移到另一个状态
4. 转移到另一个状态时，环境会根据潜在的“奖赏”函数R反馈给机器一个奖赏
5. E=<X,A,P,R>



GAN网络有两个主要组件

- 生成器
- 鉴别器

